diff --git a/action_parser.py b/action_parser.py
index 00f073e..fc8d049 100644
--- a/action_parser.py
+++ b/action_parser.py
@@ -12,7 +12,7 @@ class CustomActionParser(ContinuousAction):
         return gym.spaces.Box(-1, 1, shape=(self.action_dim,))
 
     def parse_actions(self, action_logits, state):
-        action_logits[..., 1] *= 0.1
+        action_logits[..., 1] *= 0.5
         actions = np.tanh(action_logits).reshape(-1, 2)
         filled_action = np.zeros((actions.shape[0], 8))
         filled_action[..., [0, 1]] = actions
diff --git a/actor_critic.py b/actor_critic.py
index 7485381..9464836 100644
--- a/actor_critic.py
+++ b/actor_critic.py
@@ -1,20 +1,59 @@
 import torch
+import wandb
+from stable_baselines3 import PPO
 from stable_baselines3.common.distributions import DiagGaussianDistribution
 from stable_baselines3.common.policies import ActorCriticPolicy
 from torch import nn
+from torch.distributions import Normal
+
+from callbacks import CustomLogger
+
+
+def get_model(env, use_wandb=False):
+    activation = torch.tanh if wandb.config.activation == "tanh" else torch.relu
+    model = PPO(
+        ActorCritic,
+        n_steps=4096,
+        batch_size=16,
+        env=env,
+        learning_rate=wandb.config.lr,
+        verbose=10,
+        policy_kwargs={
+            "arch": {
+                "v": wandb.config.v_arch,
+                "pi": wandb.config.pi_arch,
+                "ex": wandb.config.ex_arch
+            },
+            "log_std_init": wandb.config.log_std_init,
+            "optim": wandb.config.optim,
+            "activation": activation
+        }
+    )
+
+    model.set_logger = CustomLogger(use_wandb)
+
 
 
 class ActorCritic(ActorCriticPolicy):
-    def __init__(self, obs_space, action_space, lr=10**-5, log_std_init=1, arch=None, use_sde=False, **kwargs):
+    def __init__(self, obs_space, action_space, lr=10**-5, log_std_init=1, arch=None, use_sde=False, optim="Adam", activation=torch.nn, **kwargs):
         super(ActorCriticPolicy, self).__init__(obs_space, action_space, lr)
-        self.action_net = Net(arch=arch["pi"])
-        self.value_net = Net(arch=arch["v"])
+        self.features_extractor = Net(arch=arch["ex"], activation=activation)
+        self.action_net = Net(arch=arch["pi"], activation=activation)
+        self.value_net = Net(arch=arch["v"], activation=activation)
         self.use_sde = use_sde
         self.action_dist = DiagGaussianDistribution(action_space.shape[0])
         self.log_std = nn.Parameter(torch.ones(action_space.shape[0]) * log_std_init, requires_grad=True)
-        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr(0), **self.optimizer_kwargs)
+        if optim == "Adam":
+            self.optimizer = torch.optim.Adam(self.parameters(), lr=lr(0), **self.optimizer_kwargs)
+        elif optim == "SGD":
+            self.optimizer = torch.optim.SGD(self.parameters(), lr=lr(0), **self.optimizer_kwargs)
+        print(f"Setting up Otpim: {optim}\n"
+              f"lr: {lr}\n"
+              f"other params: {self.optimizer_kwargs}\n"
+              f"GL HF! :)")
 
-    def forward(self, x, deterministic=False, use_sde=False):
+    def forward(self, obs, deterministic=False, use_sde=False):
+        x = self.features_extractor(obs)
         values = self.value_net(x)
         mean_action = self.action_net(x)
         distribution = self.action_dist.proba_distribution(mean_action, self.log_std)
@@ -23,34 +62,38 @@ class ActorCritic(ActorCriticPolicy):
 
         return actions, values, log_prob
 
-    def evaluate_actions(self, x, actions):
-        distribution = self._get_action_dist_from_latent(x)
+    def evaluate_actions(self, obs, actions):
+        distribution = self.action_dist.proba_distribution(actions, self.log_std)
         log_prob = distribution.log_prob(actions)
+        x = self.feature_dist(obs)
         values = self.value_net(x)
 
         return values, log_prob, distribution.entropy()
 
-    def predict(self, observation, state=None, mask=None, deterministic=False):
+    def predict(self, obs, state=None, mask=None, deterministic=False):
         with torch.no_grad():
-            observation = torch.Tensor(observation)
-            actions, _, _ = self.forward(observation, deterministic=deterministic)
+            obs = torch.Tensor(obs)
+            x = self.features_extractor(obs)
+            actions = self.action_net(x)
 
         return actions, state
 
 
 class Net(nn.Module):
-    def __init__(self, arch):
+    def __init__(self, arch, activation):
         super().__init__()
         self.layers = nn.ModuleList(
             [
                 nn.Linear(arch[i-1], arch[i])for i in range(len(arch))[1:]
             ]
         )
+        self.activation = activation
 
     def forward(self, x):
+        x = x.float()
         for layer in self.layers[:-1]:
             x = layer(x)
-            x = torch.tanh(x)
+            x = self.activation(x)
         x = self.layers[-1](x)
         return x
 
diff --git a/callbacks.py b/callbacks.py
new file mode 100644
index 0000000..bc32a21
--- /dev/null
+++ b/callbacks.py
@@ -0,0 +1,20 @@
+import wandb
+from stable_baselines3.common.logger import make_output_format, KVWriter, Logger
+
+
+class CustomLogger(Logger):
+    def __init__(self, use_wandb):
+        self.use_wandb = use_wandb
+        super(CustomLogger, self).__init__("", [make_output_format("stdout", "./logs", "")])
+
+    def dump(self, step: int = 0) -> None:
+        """
+        Write all of the diagnostics from the current iteration
+        """
+        for _format in self.output_formats:
+            if isinstance(_format, KVWriter):
+                _format.write(self.name_to_value, self.name_to_excluded, step)
+        if self.use_wandb: wandb.log(self.name_to_value)
+        self.name_to_value.clear()
+        self.name_to_count.clear()
+        self.name_to_excluded.clear()
diff --git "a/jhdf\303\244.py" "b/jhdf\303\244.py"
index 7d097ef..63f8271 100644
--- "a/jhdf\303\244.py"
+++ "b/jhdf\303\244.py"
@@ -1,8 +1,13 @@
-import numpy as np
+from gym.spaces import Box
+from stable_baselines3 import PPO
+from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm
+from stable_baselines3.common.policies import register_policy, ActorCriticPolicy, get_policy_from_name
 
-actions = np.array([1, 2])
 
-
-actions = actions.reshape(-1, 2)
-filled_action = np.zeros((actions.shape[0], 8))
-filled_action[:, [0, 1]] = actions[..., 0], actions[..., 1]
+def dingi(_):
+    return 0
+dings = get_policy_from_name(ActorCriticPolicy, "MlpPolicy")(Box(0, 1, shape=(2,)), Box(0, 1, shape=(2,)), dingi)
+print(dings.features_extractor)
+print(dings.policy_net)
+print(dings.value_net)
+print()
diff --git a/match_creator.py b/match_creator.py
new file mode 100644
index 0000000..ffc83d0
--- /dev/null
+++ b/match_creator.py
@@ -0,0 +1,22 @@
+from rlgym.envs import Match
+
+
+class MatchCreator:
+    def __init__(self, state_setter, reward, obs_builder, action_parser, terminal_conds, game_speed=100):
+        self.state_setter = state_setter
+        self.reward = reward
+        self.obs_builder = obs_builder
+        self.action_parser = action_parser
+        self.terminal_conds = terminal_conds
+        self.game_speed = game_speed
+
+    def get_match(self):
+        return Match(
+             self.reward,
+             self.terminal_conds,
+             self.obs_builder,
+             self.action_parser,
+             self.state_setter,
+             game_speed=self.game_speed,
+             tick_skip=8
+        )
diff --git a/models/v0_small/best_model.zip b/models/v0_small/best_model.zip
index d77f410..5525539 100644
Binary files a/models/v0_small/best_model.zip and b/models/v0_small/best_model.zip differ
diff --git a/obs_builder.py b/obs_builder.py
index 2e06684..7c44bd8 100644
--- a/obs_builder.py
+++ b/obs_builder.py
@@ -11,7 +11,7 @@ GOAL_POSITION = np.array([0, 4096, 0])
 class CustomObsBuilder(ObsBuilder):
     def __init__(self, pos_coef=1 / 2300, ang_coef=1 / math.pi, lin_vel_coef=1 / 2300, ang_vel_coef=1 / math.pi):
         super().__init__()
-        self.obs_dim = 14
+        self.obs_dim = 10
         self.POS_COEF = pos_coef
         self.ANG_COEF = ang_coef
         self.LIN_VEL_COEF = lin_vel_coef
@@ -29,16 +29,15 @@ class CustomObsBuilder(ObsBuilder):
         player_car = player.inverted_car_data if inverted else player.car_data
 
         obs = [
-            ball.position[:2] * self.POS_COEF,
-            ball.linear_velocity[:2] * self.LIN_VEL_COEF,
-            player_car.position[:2] * self.POS_COEF,
-            player_car.forward()[:2],
-            player_car.up()[:2],
-            player_car.linear_velocity[:2] * self.LIN_VEL_COEF,
-            player_car.angular_velocity[:2] * self.ANG_VEL_COEF
+            rlm.quat_to_euler(player_car.quaternion).sum(),
+            player_car.angular_velocity.sum() * self.ANG_VEL_COEF
         ]
+        obs.extend(ball.position[:2] * self.POS_COEF)
+        obs.extend(ball.linear_velocity[:2] * self.LIN_VEL_COEF)
+        obs.extend(player_car.position[:2] * self.POS_COEF)
+        obs.extend(player_car.linear_velocity[:2] * self.LIN_VEL_COEF)
 
-        return np.concatenate(obs)
+        return np.array(obs)
 
 
 def get_polar(diff_vec):
@@ -46,7 +45,7 @@ def get_polar(diff_vec):
     y = diff_vec[1]
     if x == 0: alpha = 0
     else: alpha = np.arctan(y / x)
-    dist = (x**2 + y**2)**(1/2)
+    dist = x**2 + y**2
     return dist, alpha
 
 
@@ -55,19 +54,16 @@ class PolarObsBuilder(CustomObsBuilder):
         super().__init__()
         self.obs_dim = 6
 
-    def get_observation_space(self):
-        return gym.spaces.Box(-10, +10, shape=(self.obs_dim,))
-
     def build_obs(self, player, state, previous_action):
         inverted = player.team_num == common_values.ORANGE_TEAM
         ball = state.inverted_ball if inverted else state.ball
         player_car = player.inverted_car_data if inverted else player.car_data
 
-        player_angle = rlm.quat_to_euler(player_car.quaternion)[1]
-        player_linear_vel = np.sqrt(np.sum(player_car.linear_velocity ** 2))
+        player_angle = rlm.quat_to_euler(player_car.quaternion).sum()
+        player_linear_vel = np.sum(player_car.linear_velocity ** 2)
         player_ang_vel = player_car.angular_velocity.sum()
 
-        ball_vel = np.sqrt(np.sum(ball.linear_velocity ** 2))
+        ball_vel = np.sum(ball.linear_velocity ** 2)
 
         car_ball_vec = player_car.position - ball.position
         car_ball_dist, car_ball_ang = get_polar(car_ball_vec)
diff --git a/reward.py b/reward.py
index d4a1700..2df163d 100644
--- a/reward.py
+++ b/reward.py
@@ -6,6 +6,10 @@ from rlgym.utils.common_values import CEILING_Z, BALL_MAX_SPEED, CAR_MAX_SPEED,
     BLUE_GOAL_CENTER, ORANGE_GOAL_BACK, ORANGE_GOAL_CENTER, BALL_RADIUS, ORANGE_TEAM
 from rlgym.utils.math import cosine_similarity
 
+BLUE_GOAL = (np.array(BLUE_GOAL_BACK) + np.array(BLUE_GOAL_CENTER)) / 2
+ORANGE_GOAL = (np.array(ORANGE_GOAL_BACK) + np.array(ORANGE_GOAL_CENTER)) / 2
+MAX_DRIVING_SPEED = 1410
+
 
 class Reward(RewardFunction):
     def __init__(self, own_goal=False):
@@ -14,33 +18,37 @@ class Reward(RewardFunction):
         self.liu = LiuDistanceBallToGoalReward()
         self.vel = VelocityPlayerToBallReward()
         # self.boost = SaveBoostReward()
-        self.goal = EventReward(team_goal=1., concede=-2., touch=500., shot=10000., save=0., demo=0.)
+        # self.goal = EventReward(team_goal=1., concede=-2., touch=5., shot=10., save=0., demo=0.)
         # self.driving_to_ball = FaceBallReward()
+        self.last_state_quality = None
 
     def get_reward(self, player, state, previous_action):
-        goal_reward = self.goal.get_reward(player, state, previous_action)
+        current_quality = self.get_state_quality(player, state, previous_action)
+        reward = current_quality - self.last_state_quality if self.last_state_quality else 0
+        self.last_state_quality = current_quality
+        # reward += self.goal.get_reward(player, state, previous_action)
+
+        return reward
+
+    def get_state_quality(self, player, state, previous_action):
         liu_reward = self.liu.get_reward(player, state, previous_action)
         vel_reward = self.vel.get_reward(player, state, previous_action)
         # boost_reward = self.boost.get_reward(player, state, previous_action)
         # driving_reward = -self.driving_to_ball.get_reward(player, state, previous_action) # Minus weil wegen ka
 
-        reward = goal_reward + .2 * liu_reward + .2 * vel_reward  # + .1 * driving_reward#.0 * boost_reward +
-        reward = (reward / 0.4 - 1) / 2000
-
-        return reward
+        quality =  liu_reward + vel_reward  # + .1 * driving_reward#.0 * boost_reward +
+        return quality
 
     def reset(self, initial_state):
+        self.last_state_quality = None
         self.liu.reset(initial_state)
         self.vel.reset(initial_state)
         # self.boost.reset(initial_state)
-        self.goal.reset(initial_state)
+        # self.goal.reset(initial_state)
         # self.driving_to_ball.reset(initial_state)
 
 
 class NectoRewardFunction(RewardFunction):
-    BLUE_GOAL = (np.array(BLUE_GOAL_BACK) + np.array(BLUE_GOAL_CENTER)) / 2
-    ORANGE_GOAL = (np.array(ORANGE_GOAL_BACK) + np.array(ORANGE_GOAL_CENTER)) / 2
-
     def __init__(
             self,
             team_spirit=0.3,
@@ -49,9 +57,9 @@ class NectoRewardFunction(RewardFunction):
             goal_speed_bonus_w=2.5,
             goal_dist_bonus_w=2.5,
             demo_w=5,
-            dist_w=0.75,  # Changed from 1
+            dist_w=0.75,
             align_w=0.5,
-            boost_w=1,
+            boost_w=0, # erstmal auf 0 -> kein boost wird benutzt mensch
             touch_height_w=1,
             touch_accel_w=0.25,
             opponent_punish_w=1
@@ -75,10 +83,10 @@ class NectoRewardFunction(RewardFunction):
         self.player_qualities = None
         self.rewards = None
 
-    def _state_qualities(self, state):
+    def calculate_state_qualities(self, state):
         ball_pos = state.ball.position
-        state_quality = 0.5 * self.goal_dist_w * (np.exp(-np.linalg.norm(self.ORANGE_GOAL - ball_pos) / CAR_MAX_SPEED)
-                                                  - np.exp(-np.linalg.norm(self.BLUE_GOAL - ball_pos) / CAR_MAX_SPEED))
+        state_quality = 0.5 * self.goal_dist_w * (np.exp(-np.linalg.norm(ORANGE_GOAL - ball_pos) / CAR_MAX_SPEED)
+                                                  - np.exp(-np.linalg.norm(BLUE_GOAL - ball_pos) / CAR_MAX_SPEED))
         player_qualities = np.zeros(len(state.players))
         for i, player in enumerate(state.players):
             pos = player.car_data.position
@@ -88,18 +96,16 @@ class NectoRewardFunction(RewardFunction):
                                - cosine_similarity(ball_pos - pos, BLUE_GOAL_BACK - pos))
             if player.team_num == ORANGE_TEAM:
                 alignment *= -1
-            liu_dist = np.exp(-np.linalg.norm(ball_pos - pos) / 1410)  # Max driving speed
+            liu_dist = np.exp(-np.linalg.norm(ball_pos - pos) / MAX_DRIVING_SPEED)  # Max driving speed
             player_qualities[i] = (self.dist_w * liu_dist + self.align_w * alignment
                                    + self.boost_w * np.sqrt(player.boost_amount))
 
-            # TODO use only dist of closest player for entire team
-
         # Half state quality because it is applied to both teams, thus doubling it in the reward distributing
         return state_quality / 2, player_qualities
 
     def _calculate_rewards(self, state):
         # Calculate rewards, positive for blue, negative for orange
-        state_quality, player_qualities = self._state_qualities(state)
+        state_quality, player_qualities = self.calculate_state_qualities(state)
         player_rewards = np.zeros_like(player_qualities)
 
         for i, player in enumerate(state.players):
@@ -181,7 +187,7 @@ class NectoRewardFunction(RewardFunction):
         self.last_state = None
         self.rewards = None
         self.current_state = initial_state
-        self.state_quality, self.player_qualities = self._state_qualities(initial_state)
+        self.state_quality, self.player_qualities = self.calculate_state_qualities(initial_state)
 
     def get_reward(self, player, state, previous_action):
         if state != self.current_state:
diff --git a/state_setter.py b/state_setter.py
index 22602f1..3c4db84 100644
--- a/state_setter.py
+++ b/state_setter.py
@@ -1,18 +1,45 @@
 from rlgym.utils.state_setters import RandomState
 from numpy import random as rand
-
+from rlgym.utils.math import rand_vec3
+from rlgym.utils.state_setters.random_state import YAW_MAX
 
 X_MAX = 7000
 Y_MAX = 9000
 
 
 class StateSetter(RandomState):
-    def __init__(self):
-        super().__init__(ball_rand_speed=False, cars_rand_speed=False, cars_on_ground=True)
+    def __init__(self, x_std=1000, y_std=1000):
+        super().__init__()
+        self.x_std = x_std
+        self.y_std = y_std
+        self.car_pos = None
+
+    def reset(self, state_wrapper):
+        self._reset_cars_random(state_wrapper, self.cars_on_ground, self.cars_rand_speed)
+        self._reset_ball_random(state_wrapper, self.ball_rand_speed)
 
     def _reset_ball_random(self, state_wrapper, random_speed=False):
         state_wrapper.ball.set_pos(
-            rand.random() * X_MAX - X_MAX/2,
-            rand.random() * Y_MAX - Y_MAX/2,
+            rand.normal(self.car_pos[0], self.x_std, size=(1,)).clip(-X_MAX/2, +X_MAX/2),# * X_MAX - X_MAX/2,
+            rand.normal(self.car_pos[1], self.y_std, size=(1,)).clip(-Y_MAX/2, +Y_MAX/2),
             100
         )
+
+    def _reset_cars_random(self, state_wrapper, on_ground, random_speed):
+        """
+        Function to set all cars to a random position.
+
+        :param state_wrapper: StateWrapper object to be modified.
+        :param on_ground: Boolean indicating whether to place cars only on the ground.
+        :param random_speed: Boolean indicating whether to randomize velocity values.
+        """
+        for car in state_wrapper.cars:
+            x = rand.random() * X_MAX - X_MAX/2
+            y = rand.random() * Y_MAX - Y_MAX/2
+            self.car_pos = x, y
+            # z=17 -> on ground
+            car.set_pos(x, y, 17)
+            car.set_rot(0, rand.random() * YAW_MAX - YAW_MAX/2, 0)
+
+            car.boost = 0
+            # car.set_lin_vel(*rand_vec3(2300))
diff --git a/train.py b/train.py
index ad92a42..36379de 100644
--- a/train.py
+++ b/train.py
@@ -1,57 +1,53 @@
-import rlgym
-import torch.cuda
+import torch
 from rlgym.utils.terminal_conditions.common_conditions import NoTouchTimeoutCondition, GoalScoredCondition
+from rlgym_tools.sb3_utils import SB3MultipleInstanceEnv
 from stable_baselines3 import PPO
 from stable_baselines3.common.callbacks import EvalCallback
+from stable_baselines3.common.vec_env import SubprocVecEnv, VecNormalize, VecMonitor
+import wandb
 
 from obs_builder import CustomObsBuilder, PolarObsBuilder
 from action_parser import CustomActionParser
 from reward import Reward, NectoRewardFunction
-from actor_critic import ActorCritic
+from actor_critic import ActorCritic, get_model
 from state_setter import StateSetter
+from match_creator import MatchCreator
 
-device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
 
-obs_builder = PolarObsBuilder() #CustomObsBuilder
-action_parser = CustomActionParser()
-obs_space = obs_builder.obs_dim
-action_space = action_parser.action_dim
-state_setter = StateSetter()
+if __name__ == "__main__":
+    use_wandb = True
+    if use_wandb: wandb.init(project="stauby")
 
-# HyperParams
-reward = Reward()#NectoRewardFunction()
-lr = 1e-4
-log_std_init = 0.
-policy_kwargs = {
-    "arch": {
-        "v":  [obs_space, 4, 4, 1],
-        "pi": [obs_space, 4, 4,  action_space]
-    },
-    "log_std_init": log_std_init
-}
-
-env = rlgym.make(
-    reward_fn=reward,
-    game_speed=80,
-    terminal_conditions=(NoTouchTimeoutCondition(500), GoalScoredCondition()),
-    self_play=False,
-    state_setter=state_setter,
-    action_parser=action_parser,
-    obs_builder=obs_builder,
-    use_injector=False,
-    force_paging=False
+    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+    parallel_matches = 1
+    obs_builder = CustomObsBuilder()  #PolarObsBuilder()  #
+    action_parser = CustomActionParser()
+    obs_space = obs_builder.obs_dim
+    action_space = action_parser.action_dim
+    match_creator = MatchCreator(
+        state_setter=StateSetter(),
+        reward=Reward(),#NectoRewardFunction(),
+        obs_builder=obs_builder,
+        action_parser=action_parser,
+        terminal_conds=(NoTouchTimeoutCondition(500), GoalScoredCondition()),
+        game_speed=100
     )
+    wandb.config = {
+        "learning_rate": 1e-3,
+        "ex_arch": [obs_space, 64, 64, 8],
+        "pi_arch":  [8, 1],
+        "v_arch": [8, action_space],
+        "activation": "tanh",
+        "log_std_init": 0.,
+        "optim": "SGD"
+    }
 
-model = PPO(
-    ActorCritic,
-    # n_steps=2048,
-    env=env,
-    policy_kwargs=policy_kwargs,
-    learning_rate=lr,
-    device=device,
-    verbose=10
-    )
+    # HyperParams
+    env = SB3MultipleInstanceEnv(match_creator.get_match, parallel_matches, force_paging=False, wait_time=30)
+    env.reward_range = (-float("inf"), float("inf"))
+    env = VecNormalize(env, norm_obs=False, norm_reward=False)
+    env = VecMonitor(env)
+    model = get_model(env, use_wandb=use_wandb)
 
-callback = EvalCallback(env, best_model_save_path="models/v0_small", deterministic=False)
-model.learn(total_timesteps=1e7, callback=callback)
-model.save("models/v0_2_1")
+    model.learn(total_timesteps=1e7)
+    model.save("models/v0_2_1")
